import numpy as np
import torch
import json
from tqdm import tqdm
import torch.nn.functional as F
import argparse
import os

# Argument Parsing
parser = argparse.ArgumentParser(description="Evaluate text-video similarity.")
parser.add_argument("--feature_path", type=str, required=True, help="Path to features .npy file")
parser.add_argument("--metadata_path", type=str, required=True, help="Path to metadata .json file")
args = parser.parse_args()

# âœ… GPU ì„¤ì •
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")

# âœ… ë°ì´í„° íŒŒì¼ ë¡œë“œ
text_features = torch.tensor(np.load(os.path.join(args.feature_path, "text_features.npy"), mmap_mode="r"), dtype=torch.float32).to(device)
video_features = torch.tensor(np.load(os.path.join(args.feature_path, "video_features.npy"), mmap_mode="r"), dtype=torch.float32).to(device)

# âœ… ë¹„ë””ì˜¤ ë° í…ìŠ¤íŠ¸ ë©”íƒ€ë°ì´í„° ë¡œë“œ
with open(os.path.join(args.metadata_path, "video_metadata.json"), "r") as f:
    video_metadata = json.load(f)
with open(os.path.join(args.metadata_path, "text_metadata.json"), "r") as f:
    text_metadata = json.load(f)

# âœ… ë¹„ë””ì˜¤ë³„ í›„ë³´ êµ¬ê°„ ì •ë³´ ì €ì¥
# video_segments = {vid: torch.tensor(meta["scene_segments"][:-1], device=device) for vid, meta in video_metadata.items()}
video_segments = {vid: torch.tensor(meta["proposals"], device=device) for vid, meta in video_metadata.items()}

# âœ… í•˜ì´í¼íŒŒë¼ë¯¸í„°
text_batch_size = 100
video_batch_size = 5000
top_k_list = [1, 10, 100]

# âœ… ì •ë‹µ ë¹„ë””ì˜¤ ë§¤í•‘
query_to_gt = {qid: meta["vid"] for qid, meta in text_metadata.items()}

# âœ… Top-K í‰ê°€ ê²°ê³¼ ì €ì¥
correct_top_k = {k: 0 for k in top_k_list}
total_queries = len(text_metadata)
processed_queries = 0

# âœ… ëª¨ë“  í…ìŠ¤íŠ¸ë¥¼ ë¯¸ë‹ˆë°°ì¹˜ ë‹¨ìœ„ë¡œ ì²˜ë¦¬
print("ğŸ”¹ Processing Text Batches...")
text_progress = tqdm(range(0, total_queries, text_batch_size), desc="Text Batches", unit="batch")

for text_start in text_progress:
    text_end = min(text_start + text_batch_size, total_queries)

    # ğŸ”¹ í…ìŠ¤íŠ¸ ë°°ì¹˜ ë¶ˆëŸ¬ì˜¤ê¸°
    text_batch = text_features[text_start:text_end].to(device)
    query_ids = list(text_metadata.keys())[text_start:text_end]

    # ğŸ”¹ ìœ ì‚¬ë„ í–‰ë ¬ ì´ˆê¸°í™”
    similarity_matrix = torch.zeros((text_batch.shape[0], video_features.shape[0]), device=device)

    # ğŸ”¹ ë¹„ë””ì˜¤ í”„ë ˆì„ì„ ë¯¸ë‹ˆë°°ì¹˜ ë‹¨ìœ„ë¡œ ì—°ì‚°
    for video_start in range(0, video_features.shape[0], video_batch_size):
        video_end = min(video_start + video_batch_size, video_features.shape[0])
        video_batch = video_features[video_start:video_end].to(device)

        # ğŸ”¹ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚° (í–‰ë ¬ ì—°ì‚°)
        similarity_matrix[:, video_start:video_end] = F.cosine_similarity(
            text_batch.unsqueeze(1), video_batch.unsqueeze(0), dim=2
        )

    # âœ… ëª¨ë“  queryì— ëŒ€í•´ ë¹„ë””ì˜¤ë³„ ìµœê³  í›„ë³´ êµ¬ê°„ ìœ ì‚¬ë„ ê³„ì‚° (ë²¡í„° ì—°ì‚°)
    video_ids = list(video_segments.keys())
    num_videos = len(video_ids)

    # âœ… (batch_size, num_videos) í¬ê¸°ì˜ í…ì„œë¡œ ì´ˆê¸°í™”
    best_video_scores_tensor = torch.full((similarity_matrix.shape[0], num_videos), -float("inf"), device=device)

    for vid_idx, vid in enumerate(video_ids):
        if vid not in video_metadata:
            continue  # ë©”íƒ€ë°ì´í„°ì— ì—†ëŠ” ë¹„ë””ì˜¤ëŠ” ìŠ¤í‚µ

        global_start_idx = video_metadata[vid]["start_index"]
        total_vid_start = global_start_idx
        total_vid_end = global_start_idx + video_metadata[vid]["scene_segments"][-1][0]

        # âœ… ëª¨ë“  queryì— ëŒ€í•´ í•´ë‹¹ ë¹„ë””ì˜¤ì˜ similarity_vector ì¶”ì¶œ
        video_similarities = similarity_matrix[:, total_vid_start:total_vid_end]

        # âœ… cumsumì„ í•œ ë²ˆë§Œ ê³„ì‚° (ì „ì²´ queryì— ëŒ€í•´)
        padded_video_similarities = torch.cat((torch.zeros((video_similarities.shape[0], 1), device=device), video_similarities), dim=1)
        cumsum_sim = torch.cumsum(padded_video_similarities, dim=1)

        total_sum = torch.sum(video_similarities, dim=1, keepdim=True)
        total_frames = video_similarities.shape[1]
        total_avg_similarity = total_sum / total_frames

        # âœ… ë¹„ë””ì˜¤ ë‚´ segment ìƒëŒ€ì ì¸ ì¸ë±ìŠ¤ë¡œ ë³€í™˜
        segments = video_segments[vid]
        segment_starts = segments[:, 0].unsqueeze(0)  # (1, num_segments)
        segment_ends = segments[:, 1].unsqueeze(0)  # (1, num_segments)

        # âœ… ëª¨ë“  queryì— ëŒ€í•´ segment ë‚´ë¶€ ìœ ì‚¬ë„ í‰ê·  ê³„ì‚°
        # âœ… segment_startsì™€ segment_endsê°€ ê°™ì€ ê²½ìš° ì˜ˆì™¸ì²˜ë¦¬
        zero_length_mask = (segment_ends == segment_starts)

        # âœ… zero-length segmentì—ì„œ segment_starts > 0ì¸ì§€ ì—¬ë¶€ì— ë”°ë¼ ë‹¤ë¥´ê²Œ ì²˜ë¦¬
        segment_sums_zero_length = torch.where(
            segment_starts > 0,
            cumsum_sim[:, segment_ends] - cumsum_sim[:, segment_starts - 1],  # segment_starts > 0
            cumsum_sim[:, torch.clamp(segment_ends + 1, max=cumsum_sim.shape[1] - 1)] - cumsum_sim[:, segment_starts]  # segment_starts == 0
        )

        # âœ… ì¼ë°˜ì ì¸ segment_sums ê³„ì‚° (zero-length ì•„ë‹Œ ê²½ìš°)
        segment_sums_normal = cumsum_sim[:, segment_ends] - cumsum_sim[:, segment_starts]

        # âœ… ìµœì¢… segment_sums í• ë‹¹ (zero-lengthì´ë©´ ì˜ˆì™¸ ì²˜ë¦¬ëœ ê°’ ì‚¬ìš©)
        segment_sums = torch.where(zero_length_mask, segment_sums_zero_length, segment_sums_normal)

        # âœ… segment_lengthsì—ì„œ 0 ë°©ì§€ (0ì´ë©´ 1ë¡œ ì„¤ì •)
        segment_lengths = (segment_ends - segment_starts).float()
        segment_lengths = torch.where(zero_length_mask, torch.ones_like(segment_lengths), segment_lengths)
        segment_avg_similarities = segment_sums / segment_lengths
        
        # âœ… segment ì™¸ë¶€ ìœ ì‚¬ë„ í‰ê·  ê³„ì‚°
        non_segment_lengths = total_frames - segment_lengths
        non_segment_sums = total_sum.unsqueeze(-1) - segment_sums
        non_segment_avg_similarities = torch.where(
            non_segment_lengths > 0, non_segment_sums / non_segment_lengths, torch.zeros_like(non_segment_lengths)
        )

        # âœ… ìµœì¢… Segment Score ê³„ì‚° (ë²¡í„° ì—°ì‚°)
        segment_scores = segment_avg_similarities - non_segment_avg_similarities

        # âœ… ìµœì  segment ì„ íƒ (ê° queryë§ˆë‹¤ ê°€ì¥ ë†’ì€ segment score ì„ íƒ)
        max_segment_scores = torch.max(segment_scores, dim=-1).values  # (batch_size,)

        # âœ… best_video_scores_tensorì— ì €ì¥
        best_video_scores_tensor[:, vid_idx] = max_segment_scores.squeeze(-1)  

    # âœ… GPUì—ì„œ ë°”ë¡œ Top-K ë¹„ë””ì˜¤ ì¶”ì¶œ
    _, topk_indices = torch.topk(best_video_scores_tensor, max(top_k_list), dim=1)

    # âœ… Top-K ë¹„ë””ì˜¤ ë¦¬ìŠ¤íŠ¸ ë³€í™˜
    sorted_videos_batch = [[video_ids[idx] for idx in indices] for indices in topk_indices.tolist()]

    # âœ… í‰ê°€ ìˆ˜í–‰
    for i, query_id in enumerate(query_ids):
        sorted_videos = sorted_videos_batch[i]
        for k in top_k_list:
            top_k_videos = sorted_videos[:k]
            if query_to_gt[query_id] in top_k_videos:
                correct_top_k[k] += 1

    # âœ… ì¤‘ê°„ í‰ê°€ ê²°ê³¼ ì—…ë°ì´íŠ¸
    processed_queries += len(query_ids)
    current_top_k_acc = {k: correct_top_k[k] / processed_queries for k in top_k_list}

    # âœ… tqdmì— ì •í™•ë„ ì—…ë°ì´íŠ¸
    text_progress.set_postfix({
        f"Top-{k} Acc": f"{current_top_k_acc[k]:.4f}" for k in top_k_list
    })

# âœ… ìµœì¢… í‰ê°€ ê²°ê³¼ ì¶œë ¥
print("\nâœ… Evaluation Results:")
for k in top_k_list:
    accuracy = correct_top_k[k] / total_queries
    print(f"ğŸ¯ Top-{k} Accuracy: {accuracy:.4f}")
