import torch
import numpy as np
import json
from tqdm import tqdm
import heapq

# GPU ì„¤ì • (ê°€ëŠ¥í•˜ë©´ GPU ì‚¬ìš©)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")

# ë°ì´í„° íŒŒì¼ ë¡œë“œ (mmap_mode="r"ëŠ” CPUì—ì„œë§Œ ë™ì‘í•˜ë¯€ë¡œ GPU ì‚¬ìš© ì‹œ ë©”ëª¨ë¦¬ì— ë¡œë“œ)
text_features = torch.tensor(np.load("text_features.npy"), dtype=torch.float32).to(device)  # (num_texts, feature_dim)
video_features = torch.tensor(np.load("video_features.npy"), dtype=torch.float32).to(device)  # (total_frames, feature_dim)

# ë¹„ë””ì˜¤ ë©”íƒ€ë°ì´í„° ë¡œë“œ
with open("video_metadata.json", "r") as f:
    video_metadata = json.load(f)

# í…ìŠ¤íŠ¸ ë©”íƒ€ë°ì´í„° ë¡œë“œ (query_id, ì •ë‹µ ë¹„ë””ì˜¤ ì •ë³´ í¬í•¨)
with open("text_metadata.json", "r") as f:
    text_metadata = json.load(f)

# ë¹„ë””ì˜¤ë³„ í›„ë³´ êµ¬ê°„ ì •ë³´ ì €ì¥ (vid -> [(start1, end1), (start2, end2), ...])
# video_segments = {vid: torch.tensor(meta["segments"], device=device) for vid, meta in video_metadata.items()}
video_segments = {vid: torch.tensor(meta["proposals"], device=device) for vid, meta in video_metadata.items()}

# í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ì •
text_batch_size = 100    # í…ìŠ¤íŠ¸ ë¯¸ë‹ˆë°°ì¹˜ í¬ê¸°
video_batch_size = 5000  # ë¹„ë””ì˜¤ í”„ë ˆì„ ë¯¸ë‹ˆë°°ì¹˜ í¬ê¸° (ë„ˆë¬´ í¬ë©´ ë©”ëª¨ë¦¬ ì´ˆê³¼)
top_k_list = [1, 10, 100]  # ì—¬ëŸ¬ ê°œì˜ top-k í‰ê°€ ìˆ˜í–‰

# ì •ë‹µ ë¹„ë””ì˜¤ ë§¤í•‘ (query_id -> ì •ë‹µ ë¹„ë””ì˜¤ ID)
query_to_gt = {qid: meta["vid"] for qid, meta in text_metadata.items()}

# Top-K í‰ê°€ ê²°ê³¼ ì €ì¥
correct_top_k = {k: 0 for k in top_k_list}  # {1: 0, 10: 0, 100: 0}
total_queries = len(text_metadata)

# ëª¨ë“  í…ìŠ¤íŠ¸ë¥¼ ë¯¸ë‹ˆë°°ì¹˜ ë‹¨ìœ„ë¡œ ì²˜ë¦¬ (GPU í™œìš©)
print("Processing Text Batches...")
for text_start in tqdm(range(0, total_queries, text_batch_size), desc="Text Batches"):
    text_end = min(text_start + text_batch_size, total_queries)

    # í…ìŠ¤íŠ¸ ë°°ì¹˜ ë¶ˆëŸ¬ì˜¤ê¸° (GPU ì´ë™)
    text_batch = text_features[text_start:text_end].to(device)  # (text_batch_size, feature_dim)
    query_ids = list(text_metadata.keys())[text_start:text_end]

    # ìœ ì‚¬ë„ í–‰ë ¬ ì €ì¥ (í˜„ì¬ í…ìŠ¤íŠ¸ ë°°ì¹˜ì— ëŒ€í•œ ë¹„ë””ì˜¤ ìœ ì‚¬ë„)
    similarity_matrix = torch.zeros((text_batch_size, video_features.shape[0]), device=device)

    # ë¹„ë””ì˜¤ í”„ë ˆì„ë„ ë¯¸ë‹ˆë°°ì¹˜ ë‹¨ìœ„ë¡œ ì½ìœ¼ë©° ì—°ì‚° (GPU í™œìš©)
    for video_start in range(0, video_features.shape[0], video_batch_size):
        video_end = min(video_start + video_batch_size, video_features.shape[0])
        
        # ë¹„ë””ì˜¤ í”„ë ˆì„ ë°°ì¹˜ ë¶ˆëŸ¬ì˜¤ê¸° (GPU ì´ë™)
        video_batch = video_features[video_start:video_end].to(device)  # (video_batch_size, feature_dim)

        # í˜„ì¬ í…ìŠ¤íŠ¸ ë°°ì¹˜ì™€ ë¹„ë””ì˜¤ í”„ë ˆì„ ë°°ì¹˜ ê°„ ìœ ì‚¬ë„ ê³„ì‚° (GPUì—ì„œ í–‰ë ¬ê³± ìˆ˜í–‰)
        similarity_matrix[:, video_start:video_end] = text_batch @ video_batch.T  # (text_batch_size, video_batch_size)
    
    # ê° í…ìŠ¤íŠ¸ë³„ Top-K ë¹„ë””ì˜¤ ì°¾ê¸°
    for i, query_id in enumerate(query_ids):
        similarity_vector = similarity_matrix[i]  # (total_frames,)
        
        # ğŸ”¹ ë¹„ë””ì˜¤ë³„ ìµœê³  í›„ë³´ êµ¬ê°„ ìœ ì‚¬ë„ ì°¾ê¸°
        best_video_scores = {}

        for vid, segments in video_segments.items():
            max_segment_score = -float("inf")  

            if vid not in video_metadata:
                continue  # ë©”íƒ€ë°ì´í„°ì— ì—†ëŠ” ë¹„ë””ì˜¤ëŠ” ìŠ¤í‚µ

            global_start_idx = video_metadata[vid]["start_index"]
            total_vid_start = global_start_idx
            total_vid_end = global_start_idx + video_metadata[vid]["scene_segments"][-1][0]
            
            # # âœ… ë¹„ë””ì˜¤ ì „ì²´ í‰ê·  ìœ ì‚¬ë„ ê³„ì‚° (ë²¡í„° ì—°ì‚°)
            # video_similarities = similarity_vector[total_vid_start:total_vid_end]
            # total_sum = torch.sum(video_similarities)
            # total_frames = video_similarities.numel()

            # for segment in segments:
            #     local_start, local_end = segment
            #     start_idx = global_start_idx + local_start
            #     end_idx = global_start_idx + local_end

            #     if end_idx > similarity_vector.shape[0]:
            #         continue

            #     # âœ… Segment ë‚´ë¶€ ìœ ì‚¬ë„ í‰ê·  ê³„ì‚°
            #     segment_similarities = similarity_vector[start_idx:end_idx]
            #     segment_sum = torch.sum(segment_similarities)
            #     segment_frames = segment_similarities.numel()
            #     segment_avg_similarity = segment_sum / segment_frames

            #     # âœ… Segment ì™¸ë¶€ ìœ ì‚¬ë„ í‰ê·  ê³„ì‚° (ì „ì²´ í‰ê·  í™œìš©)
            #     non_segment_frames = total_frames - segment_frames
            #     if non_segment_frames > 0:
            #         non_segment_avg_similarity = (total_sum - segment_sum) / non_segment_frames
            #     else:
            #         non_segment_avg_similarity = 0  # ì˜ˆì™¸ ì²˜ë¦¬

            #     # âœ… ìµœì¢… Segment Score = (Segment ë‚´ë¶€ í‰ê·  - Segment ì™¸ë¶€ í‰ê· )
            #     segment_score = segment_avg_similarity - non_segment_avg_similarity

            #     if segment_score > max_segment_score:
            #         max_segment_score = segment_score


            # âœ… ë¹„ë””ì˜¤ ì „ì²´ í‰ê·  ìœ ì‚¬ë„ ê³„ì‚°
            video_similarities = similarity_vector[total_vid_start:total_vid_end]
            total_sum = torch.sum(video_similarities)
            total_frames = video_similarities.numel()
            total_avg_similarity = total_sum / total_frames

            # âœ… Segmentë³„ í‰ê·  ìœ ì‚¬ë„ ë²¡í„° ì—°ì‚°
            segment_starts = global_start_idx + segments[:, 0]  # (num_segments,)
            segment_ends = global_start_idx + segments[:, 1]  # (num_segments,)

            # âœ… Segment ë‚´ë¶€ ìœ ì‚¬ë„ í‰ê·  ê³„ì‚°
            segment_sums = torch.cumsum(video_similarities, dim=0)[segment_ends - 1] - torch.cat(
                (torch.tensor([0], device=device), torch.cumsum(video_similarities, dim=0)[segment_starts - 1]))
            segment_lengths = (segment_ends - segment_starts).float()
            segment_avg_similarities = segment_sums / segment_lengths

            # âœ… Segment ì™¸ë¶€ ìœ ì‚¬ë„ í‰ê·  ê³„ì‚°
            non_segment_lengths = total_frames - segment_lengths
            non_segment_sums = total_sum - segment_sums
            non_segment_avg_similarities = torch.where(
                non_segment_lengths > 0, non_segment_sums / non_segment_lengths, torch.tensor(0, device=device)
            )

            # âœ… ìµœì¢… Segment Score ê³„ì‚°
            segment_scores = segment_avg_similarities - non_segment_avg_similarities

            # âœ… ìµœì  segment ì„ íƒ
            max_segment_score = torch.max(segment_scores).item()
            best_video_scores[vid] = max_segment_score

        # âœ… ë¹„ë””ì˜¤ ì •ë ¬ ìµœì í™” (heapq.nlargest)
        sorted_videos = heapq.nlargest(max(top_k_list), best_video_scores, key=best_video_scores.get)

        # ğŸ”¹ ê° top_kì— ëŒ€í•´ í‰ê°€
        for k in top_k_list:
            top_k_videos = sorted_videos[:k]
            if query_to_gt[query_id] in top_k_videos:
                correct_top_k[k] += 1

# âœ… ìµœì¢… í‰ê°€ ê²°ê³¼ ì¶œë ¥
print("\nâœ… Evaluation Results:")
for k in top_k_list:
    accuracy = correct_top_k[k] / total_queries
    print(f"ğŸ¯ Top-{k} Accuracy: {accuracy:.4f}")