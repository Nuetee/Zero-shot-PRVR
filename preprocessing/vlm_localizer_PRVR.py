import os
import torch
import numpy as np
from scipy.optimize import minimize_scalar
import torch.nn.functional as F
from lavis.models import load_model_and_preprocess
from torchvision import transforms
from sklearn.cluster import KMeans

#### BLIP-2 Q-Former ####
model, vis_processors, text_processors = load_model_and_preprocess("blip2_image_text_matching", "coco", device='cuda',
                                                                   is_eval=True)
vis_processors = transforms.Compose([
    t for t in vis_processors['eval'].transform.transforms if not isinstance(t, transforms.ToTensor)
])
#### BLIP-2 Q-Former ####


def iou(candidates, gt):
    start, end = candidates[:, 0], candidates[:, 1]
    s, e = gt[0].float(), gt[1].float()
    inter = end.min(e) - start.max(s)
    union = end.max(e) - start.min(s)
    return inter.clamp(min=0) / union

def gaussian_kernel(size, sigma=1):
    size = int(size) // 2
    x = np.arange(-size, size + 1)
    normal = 1 / (np.sqrt(2.0 * np.pi) * sigma)
    g = np.exp(-x ** 2 / (2.0 * sigma ** 2)) * normal
    return g

def nchk(f, f1, f2, ths):
    return (((3 * f) > ths) | ((2 * f + f1) > ths) | ((f + f1 + f2) > ths))

def get_dynamic_scores(scores, stride, masks, ths=0.0005, sigma=1):
    gstride = min(stride - 2, 3)
    if (stride < 3):
        gkernel = torch.ones((1, 1, 1)).to('cuda')
    else:
        gkernel = gaussian_kernel(gstride, sigma)
        gkernel = torch.from_numpy(gkernel).float().to('cuda')
        gkernel = gkernel.view(1, 1, -1)
    gscore = F.conv1d(scores.view(-1, 1, scores.size(-1)), gkernel).view(scores.size(0), -1)

    diffres = torch.diff(gscore).to('cuda')
    pad_left = torch.zeros((diffres.size(0), (masks.size(-1) - diffres.size(-1)) // 2)).to('cuda')
    pad_right = torch.zeros((diffres.size(0), masks.size(-1) - diffres.size(-1) - pad_left.size(-1))).to('cuda')
    diffres = torch.cat((pad_left, diffres, pad_right), dim=-1) * masks

    dynamic_scores = np.zeros((diffres.size(0), diffres.size(-1)))
    dynamic_idxs = np.zeros((diffres.size(0), diffres.size(-1)))

    for idx in range(diffres.size(0)):
        f1 = f2 = f3 = 0
        d_score = 0
        d_idx = 0
        for i in range(diffres.size(-1)):
            f3 = f2
            f2 = f1
            f1 = diffres[idx][i]
            if nchk(f1, f2, f3, ths):
                d_score += max(3 * f1, 2 * f1 + f2, f1 + f2 + f3)
            else:
                d_idx = i
                d_score = 0

            dynamic_idxs[idx][i] = d_idx / scores.size(-1)
            dynamic_scores[idx][i] = d_score

    dynamic_idxs = torch.from_numpy(dynamic_idxs).to('cuda')
    dynamic_scores = torch.from_numpy(dynamic_scores).to('cuda')
    return dynamic_idxs, dynamic_scores


def split_interval(init_timestep):
    init_timestep = init_timestep.cpu().sort()[0]
    # ê²°ê³¼ë¥¼ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸
    ranges = []

    # ì„ì‹œë¡œ ì‹œì‘ê³¼ ëì„ ì €ì¥í•  ë³€ìˆ˜
    start = init_timestep[0]
    end = init_timestep[0].clone()

    # í…ì„œì˜ ê° ì›ì†Œë¥¼ ìˆœì°¨ì ìœ¼ë¡œ ë¹„êµ
    for i in range(1, len(init_timestep)):
        if init_timestep[i] == end + 1:
            # ì—°ì†ëœ ìˆ«ìì¸ ê²½ìš°
            end = init_timestep[i]
        else:
            # ì—°ì†ë˜ì§€ ì•Šì€ ìˆ«ìê°€ ë‚˜íƒ€ë‚˜ë©´ êµ¬ê°„ì„ ì¶”ê°€í•˜ê³  ìƒˆë¡œ ì‹œì‘
            ranges.append([start, end])
            start = init_timestep[i]
            end = init_timestep[i].clone()

    # ë§ˆì§€ë§‰ êµ¬ê°„ ì¶”ê°€
    ranges.append([start, end])
    return torch.tensor(ranges)

import re
def sanitize_filename(filename):
    # í—ˆìš©ë˜ì§€ ì•ŠëŠ” ë¬¸ìë¥¼ `_`ë¡œ ëŒ€ì²´
    filename = re.sub(r'[\/:*?"<>|]', '_', filename)
    return filename


def extract_static_score(start, end, cum_scores, num_frames):
    kernel_size = end - start
    if start == 0:
        inner_sum = cum_scores[end - 1]
    else:
        inner_sum = cum_scores[end - 1] - cum_scores[start - 1]

    outer_sum = cum_scores[num_frames - 1] - inner_sum

    if kernel_size != num_frames:
        static_score = inner_sum / kernel_size - outer_sum / (num_frames - kernel_size)
    else:
        # static_score = inner_sum / kernel_size - (scores[0][0] + scores[0][-1] / 2)
        static_score = inner_sum / kernel_size
    return static_score


def scores_masking(scores, masks):
    # scoresì˜ ê¸¸ì´ê°€ 3 ë¯¸ë§Œì¸ ê²½ìš° initial_maskë¥¼ ê·¸ëŒ€ë¡œ ì‚¬ìš©
    if scores.shape[1] < 3:
        masks = masks.squeeze()
    else:
        # ì–‘ìª½ ëì— 2ì”© Falseë¡œ íŒ¨ë”©
        padded_masks = F.pad(masks, (1, 1), mode='constant', value=False)

        # í˜„ì¬ ìœ„ì¹˜ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì–‘ì˜† 2ê°œì˜ ê°’ ê¸°ë°˜ Majority voting, ìµœì¢… ë§ˆìŠ¤í¬ ê²°ê³¼ ì €ì¥
        final_masks = padded_masks.clone()
        for i in range(2, padded_masks.shape[1] - 1):
            window = padded_masks[:, i - 1 : i + 2]
            if window.sum() < 2:
                final_masks[:, i] = 0

        # íŒ¨ë”© ì œê±°í•˜ì—¬ ì›ë˜ í¬ê¸°ì˜ ë§ˆìŠ¤í¬ë¡œ ë³µì›
        masks = final_masks[:, 1:-1].squeeze()
    
    # ëª¨ë“  ê°’ì´ Falseì¼ ê²½ìš° ì „ë¶€ Trueë¡œ ì„¤ì •
    if not masks.any():
        masks[:] = True

    # final_maskë¥¼ ê¸°ë°˜ìœ¼ë¡œ masked_indices ê³„ì‚°
    masked_indices = torch.nonzero(masks, as_tuple=True)[0]  # ë§ˆìŠ¤í‚¹ëœ ì‹¤ì œ ì¸ë±ìŠ¤ ì €ì¥
    
    return masks, masked_indices


def alignment_adjustment(data, scale_gamma, device, lambda_max=2, lambda_min=-2):
    # ì‘ì€ ìƒìˆ˜ ì¶”ê°€ë¡œ ì–‘ìˆ˜ ë°ì´í„° ë³´ì¥
    epsilon = 1e-6
    data = data + abs(data.min()) + epsilon if np.any(data <= 0) else data
    
    def boxcox_transformed(x, lmbda):
        if lmbda == 0:
            return np.log(x)
        else:
            return (x**lmbda - 1) / lmbda

    # ìµœì ì˜ lambdaë¥¼ ì°¾ê¸° ìœ„í•œ ë¡œê·¸ ê°€ëŠ¥ë„ í•¨ìˆ˜ (ìµœì†Œí™”í•  í•¨ìˆ˜)
    def neg_log_likelihood(lmbda):
        transformed_data = boxcox_transformed(data, lmbda)
        # ë¶„ì‚° ê³„ì‚° ì‹œ overflow ë°©ì§€
        var = np.var(transformed_data, ddof=1)
        return -np.sum(np.log(np.abs(transformed_data))) + 0.5 * len(data) * np.log(var)

    # lambda ë²”ìœ„ ë‚´ì—ì„œ ìµœì í™”
    result = minimize_scalar(neg_log_likelihood, bounds=(lambda_min, lambda_max), method='bounded')
    best_lambda = result.x
    
    # ìµœì ì˜ lambdaë¡œ ë³€í™˜ ë°ì´í„° ìƒì„±
    transformed_data = boxcox_transformed(data, best_lambda)

    original_min, original_max = data.min(), data.max()
    transformed_min, transformed_max = transformed_data.min(), transformed_data.max()
    transformed_data = (transformed_data - transformed_min) / (transformed_max - transformed_min)  # normalize to [0, 1]
    is_scale = False
    if original_max - original_min > scale_gamma:
        is_scale = True
        transformed_data = transformed_data * (original_max - original_min) + original_min  # scale to original min/max
    else:
        transformed_data = transformed_data * (scale_gamma) + original_min
    # ë³€í™˜ ê²°ê³¼ë¥¼ ë‹¤ì‹œ í…ì„œë¡œ ë³€í™˜í•˜ê³  ì›ë˜ í˜•íƒœë¡œ ë³µì›

    normalized_scores = torch.tensor(transformed_data, device=device).unsqueeze(0)

    return normalized_scores, is_scale


# import torch.nn.functional as F
# def temporal_aware_feature_smoothing(kernel_size, video_features):
#     F_dim, Q, C = video_features.shape  # Frame, Query, Channel í¬ê¸°

#     # padding size ê³„ì‚°
#     pad_size = kernel_size // 2

#     video_features = video_features.permute(1, 2, 0)  # (Q, C, F)

#     # Frame ì°¨ì›(F)ì— ëŒ€í•´ padding ì ìš©
#     padded_features = F.pad(video_features, (pad_size, pad_size), mode='replicate')  # (Q, C, F + 2*pad_size)

#     # Mean poolingì„ Frame ì°¨ì›(F)ì—ì„œ ìˆ˜í–‰
#     pooled_features = F.avg_pool1d(padded_features, kernel_size, stride=1)  # (Q, C, F)

#     # ì›ë˜ ì°¨ì› (F, Q, C)ìœ¼ë¡œ ë³€í™˜
#     return pooled_features.permute(2, 0, 1)  # (F, Q, C)


def kmeans_clustering(k, features):
    kmeans = KMeans(n_clusters=k, n_init=10, random_state=42)
    kmeans_labels = kmeans.fit_predict(np.array(features.cpu()))
    kmeans_labels = torch.tensor(kmeans_labels)

    return kmeans_labels


def kmeans_clustering_gpu(k, features, n_iter=100, tol=1e-4):
    # Ensure features are on GPU
    torch.manual_seed(60)
    features = features.cuda().float()
    
    n_samples, n_features = features.shape

    # Initialize centroids using k-means++ algorithm
    centroids = torch.empty((k, n_features), device=features.device, dtype=torch.float32)
    # Step 1: Choose the first centroid randomly
    random_idx = torch.randint(0, n_samples, (1,))
    centroids[0] = features[random_idx]

    # Step 2: Choose remaining centroids
    for i in range(1, k):
        # Compute squared distances from the closest centroid
        distances = torch.min(torch.cdist(features, centroids[:i])**2, dim=1).values
        probabilities = distances / distances.sum()
        cumulative_probs = torch.cumsum(probabilities, dim=0)
        random_value = torch.rand(1, device=features.device)
        next_idx = torch.searchsorted(cumulative_probs, random_value).item()
        centroids[i] = features[next_idx]

    # Perform k-means clustering
    for i in range(n_iter):
        # Calculate distances (broadcasting)
        distances = torch.cdist(features, centroids, p=2)

        # Assign clusters
        labels = torch.argmin(distances, dim=1)

        # Update centroids
        new_centroids = torch.stack([features[labels == j].mean(dim=0) if (labels == j).sum() > 0 else centroids[j] for j in range(k)])

        # Check for convergence
        if torch.allclose(centroids, new_centroids, atol=tol):
            break

        centroids = new_centroids

    return labels.cpu()

def segment_scenes_by_cluster(cluster_labels):
    scene_segments = []
    start_idx = 0

    current_label = cluster_labels[0]
    for i in range(1, len(cluster_labels)):
        if cluster_labels[i] != current_label:
            scene_segments.append([start_idx, i])  ### start_idx ì´ìƒ, i ë¯¸ë§Œ ê¹Œì§€ ê°™ì€ ë ˆì´ë¸”
            start_idx = i
            current_label = cluster_labels[i]
    
    scene_segments.append([start_idx, len(cluster_labels)])
    scene_segments.append([len(cluster_labels), len(cluster_labels)])

    return scene_segments

def get_proposals_with_scores2(scene_segments, num_frames, prior):
    proposals = []
    for i in range(len(scene_segments)):
        for j in range(i + 1, len(scene_segments)):
            start = scene_segments[i][0]
            last = scene_segments[j][0]
            if (last - start) > num_frames * prior:
                continue
            
            proposals.append([start, last])

    return proposals


def temporal_aware_feature_smoothing(kernel_size, features):
    padding_size = kernel_size // 2
    padded_features = torch.cat((features[0].repeat(padding_size, 1), features, features[-1].repeat(padding_size, 1)), dim=0)
    kernel = torch.ones(padded_features.shape[1], 1, kernel_size).cuda() / kernel_size
    padded_features = padded_features.unsqueeze(0).permute(0, 2, 1)  # (1, 257, 104)
    padded_features = padded_features.float()

    temporal_aware_features = F.conv1d(padded_features, kernel, padding=0, groups=padded_features.shape[1])
    temporal_aware_features = temporal_aware_features.permute(0, 2, 1)
    temporal_aware_features = temporal_aware_features[0]

    return temporal_aware_features


def generate_segments(video_features, hyperparams, kmeans_gpu):
    num_frames = video_features.shape[0]

    video_features = torch.tensor(video_features).cuda()
    if hyperparams['is_blip2']:
        video_features = video_features.mean(dim=1) 
        
    # Temporal-aware vector smoothing
    temporal_aware_features = temporal_aware_feature_smoothing(hyperparams['temporal_window_size'], video_features)

    # Kmeans Clustering
    kmeans_k = min(hyperparams['kmeans_k'], max(2, num_frames))
    if kmeans_gpu:
        kmeans_labels = kmeans_clustering_gpu(kmeans_k, temporal_aware_features)
    else:
        kmeans_labels = kmeans_clustering(kmeans_k, temporal_aware_features)
    
    # Kmeans clusetring ê²°ê³¼ì— ë”°ë¼ ë¹„ë””ì˜¤ ì¥ë©´ Segmentation
    scene_segments = segment_scenes_by_cluster(kmeans_labels)
    proposals = get_proposals_with_scores2(scene_segments, num_frames, hyperparams['prior'])

    return scene_segments, proposals, num_frames


def get_proposals_with_scores(scene_segments, cum_scores, cum_norm_scores, num_frames, prior):
    proposals = []
    proposals_inner_mean = []
    proposals_norm_scores = []
    for i in range(len(scene_segments)):
        for j in range(i + 1, len(scene_segments)):
            start = scene_segments[i][0]
            last = scene_segments[j][0]
            if (last - start) > num_frames * prior:
                continue
            score_norm = extract_static_score(start, last, cum_norm_scores, len(cum_norm_scores)).item()
            
            kernel_size = last - start
            if start == 0:
                inner_sum = cum_scores[last - 1]
            else:
                inner_sum = cum_scores[last - 1] - cum_scores[start - 1]
            
            inner_mean = inner_sum / kernel_size
            inner_mean = inner_mean.item()
            
            proposals.append([start, last])
            proposals_norm_scores.append(round(score_norm, 4))
            proposals_inner_mean.append(round(inner_mean, 4))

    return proposals, proposals_norm_scores, proposals_inner_mean


def video_retrieval(data, feature_path, query, stride, hyperparams, kmeans_gpu):
    video_scores = []
    with torch.no_grad():
        text = model.tokenizer(query, padding='max_length', truncation=True, max_length=35, return_tensors="pt").to(
            'cuda')
        text_output = model.Qformer.bert(text.input_ids, attention_mask=text.attention_mask, return_dict=True)
        text_feat = model.text_proj(text_output.last_hidden_state[:, 0, :])
    v1 = F.normalize(text_feat, dim=-1)

    for vid, ann in data.items():
        video_feature = np.load(os.path.join(feature_path, vid+'.npy'))
        num_frames = video_feature.shape[0]
        duration = ann['duration']

        v2 = F.normalize(torch.tensor(video_feature, device='cuda', dtype=v1.dtype), dim=-1)
        scores = torch.einsum('md,npd->mnp', v1, v2)
        scores, scores_idx = scores.max(dim=-1)
        scores = scores.mean(dim=0, keepdim=True)
        # scores > 0.2ì¸ ë§ˆìŠ¤í‚¹ ìƒì„± (Boolean í˜•íƒœ ìœ ì§€)
        initial_masks = (scores > 0.2 if hyperparams['is_blip2'] else scores > 0)
        masks, masked_indices = scores_masking(scores, initial_masks)

        # Alignment adjustment of similarity scores
        data = scores[:, masks].flatten().cpu().numpy()   # ë§ˆìŠ¤í¬ëœ ë¶€ë¶„ë§Œ ê°€ì ¸ì˜¤ê¸°    
        normalized_scores, is_scale = alignment_adjustment(data, hyperparams['gamma'], scores.device, lambda_max=2, lambda_min=-2)

        # ë§ˆìŠ¤í‚¹ëœ ë¶€ë¶„ì„ 0ìœ¼ë¡œ ì±„ìš´ ì›ë³¸ í¬ê¸°ì˜ scores í…ì„œ ìƒì„±
        restored_scores = torch.zeros_like(scores)  # ì›ë³¸ scores í¬ê¸°ë¡œ ì´ˆê¸°í™”

        # ë§ˆìŠ¤í‚¹ëœ ë¶€ë¶„ë§Œ normalized_scores ê°’ìœ¼ë¡œ ì±„ìš°ê¸°
        restored_scores[:, masks] = normalized_scores.clone().detach()
        cum_norm_scores = torch.cumsum(restored_scores, dim=1)[0]
        cum_scores = torch.cumsum(scores, dim=1)[0]

        scene_segments = ann['scene_segments']
        final_proposals, final_proposals_score, final_proposals_inner_mean = get_proposals_with_scores(scene_segments, cum_scores, cum_norm_scores, num_frames, hyperparams['prior'])
        
        final_proposals = torch.tensor(final_proposals)
        final_proposals_score = torch.tensor(final_proposals_score)
        final_proposals_inner_mean = torch.tensor(final_proposals_inner_mean)
        _, score_index = final_proposals_score.sort(descending=True)
        final_proposals = final_proposals[score_index]
        final_proposals_scores = final_proposals_score[score_index]
        final_proposals_inner_mean = final_proposals_inner_mean[score_index]

        #### dynamic scoring #####
        masked_scores = scores * initial_masks.float()
        stride = min(stride, masked_scores.size(-1) // 2)

        dynamic_idxs, dynamic_scores = get_dynamic_scores(masked_scores, stride, initial_masks.float())
        dynamic_frames = torch.round(dynamic_idxs * num_frames).int()
        
        for final_proposal in final_proposals:
            current_frame = final_proposal[0]
            dynamic_prefix = dynamic_frames[0][current_frame]
            while True:
                if current_frame == 0 or dynamic_frames[0][current_frame - 1] != dynamic_prefix:
                    break
                current_frame -= 1
            final_proposal[0] = current_frame

        #### dynamic scoring #####
        
        if len(final_proposals) == 0:
            static_pred = np.array([[0.0, 0.0], [0.0, 0.0], [0.0, 0.0]])
            scores = np.array([1.0, 1.0, 1.0])
        else:
            static_pred = final_proposals / num_frames * duration
            scores = final_proposals_scores
            scores = scores / scores.max()

        proposals = []
        for s_pred, score, inner_mean in zip(static_pred, scores, final_proposals_inner_mean):
            proposals.append([
                float(s_pred[0]),
                float(s_pred[1]),
                float(score),
                float(inner_mean)
            ])

        proposals = proposals[:hyperparams['cand_num']]

        def calc_iou(candidates, gt):
            start, end = candidates[:,0], candidates[:,1]
            s, e = gt[0], gt[1]
            inter = np.minimum(end, e) - np.maximum(start, s)
            union = np.maximum(end, e) - np.minimum(start, s)
            return inter.clip(min=0) / union
        
        def select_proposal(inputs, gamma=0.6):
            weights = inputs[:, 2].clip(min=0)
            proposals = inputs[:, :2]
            scores = np.zeros_like(weights)

            for j in range(scores.shape[0]):
                iou = calc_iou(proposals, proposals[j])
                scores[j] += (iou ** gamma * weights).sum()

            idx = np.argsort(-scores)
            return inputs[idx]

        proposals = select_proposal(np.array(proposals))
        video_scores.append({
            'vid': vid,
            'score': proposals[0][-1]
        })
    
    sorted_vids = [item['vid'] for item in sorted(video_scores, key=lambda x: x['score'], reverse=True)]

    return sorted_vids


def get_proposals_with_scores_batch(scene_segments, cum_scores, num_frames, prior):
    proposals = []
    proposals_static_scores = []
    proposals_inner_mean = []
    for i in range(len(scene_segments)):
        for j in range(i + 1, len(scene_segments)):
            start = scene_segments[i][0]
            last = scene_segments[j][0]
            if (last - start) > num_frames * prior:
                continue
            score_static = extract_static_score(start, last, cum_scores, len(cum_scores)).item()

            kernel_size = last - start
            if start == 0:
                inner_sum = cum_scores[last - 1]
            else:
                inner_sum = cum_scores[last - 1] - cum_scores[start - 1]
            
            inner_mean = inner_sum / kernel_size
            inner_mean = inner_mean.item()
            
            proposals.append([start, last])
            proposals_static_scores.append(round(score_static, 4))
            proposals_inner_mean.append(round(inner_mean, 4))

    return proposals, proposals_static_scores, proposals_inner_mean


def video_retrieval_batch(data, feature_path, query, stride, hyperparams, batch_size=32):
    all_results = []

    # 1ï¸âƒ£ Queryë¥¼ BLIP-2 ëª¨ë¸ë¡œ ì²˜ë¦¬ (í•œ ë²ˆë§Œ ìˆ˜í–‰)
    with torch.no_grad():
        text = model.tokenizer(query, padding='max_length', truncation=True, max_length=35, return_tensors="pt").to('cuda')
        text_output = model.Qformer.bert(text.input_ids, attention_mask=text.attention_mask, return_dict=True)
        text_feat = model.text_proj(text_output.last_hidden_state[:, 0, :])  # (1, D)
        text_feat = F.normalize(text_feat, dim=-1)  # (1, D)
    
    # 2ï¸âƒ£ ë°ì´í„°ì…‹ì„ batch_size ë‹¨ìœ„ë¡œ ì²˜ë¦¬
    data_items = list(data.items())
    num_batches = (len(data_items) + batch_size - 1) // batch_size  # ì˜¬ë¦¼ ì²˜ë¦¬
    
    for batch_idx in range(num_batches):
        batch_data = data_items[batch_idx * batch_size : (batch_idx + 1) * batch_size]
        video_features = []
        video_meta = []  # (vid, duration, scene_segments)
        frame_counts = []

        # 3ï¸âƒ£ í˜„ì¬ batchì˜ ë¹„ë””ì˜¤ feature ë¡œë“œ
        for vid, ann in batch_data:
            feature = np.load(os.path.join(feature_path, vid + '.npy'))  # (F, Q, D)
            video_features.append(torch.tensor(feature, dtype=torch.float32, device='cuda'))
            frame_counts.append(feature.shape[0])  # ê° ë¹„ë””ì˜¤ì˜ ì‹¤ì œ Frame ìˆ˜ ì €ì¥
            video_meta.append((vid, ann['duration'], ann['scene_segments']))
        
        # 4ï¸âƒ£ Padding: ê°€ì¥ ê¸´ ë¹„ë””ì˜¤ì˜ Frame ìˆ˜ì— ë§ì¶¤
        max_F = max(frame_counts)  # ë°°ì¹˜ ë‚´ ê°€ì¥ ê¸´ ë¹„ë””ì˜¤ì˜ Frame ìˆ˜
        padded_video_features = []
        padding_masks = []

        for i, feature in enumerate(video_features):
            F_c, Q, D = feature.shape
            pad_size = max_F - F_c

            # ë’¤ìª½ì— íŒ¨ë”© ì¶”ê°€ (zero-padding)
            padded_feature = F.pad(feature, (0, 0, 0, 0, 0, pad_size), mode='constant', value=0)
            padded_video_features.append(padded_feature)

            # ì‹¤ì œ í”„ë ˆì„ ì—¬ë¶€ë¥¼ ë‚˜íƒ€ë‚´ëŠ” mask ìƒì„±
            mask = torch.cat([torch.ones(F_c, device='cuda'), torch.zeros(pad_size, device='cuda')])
            padding_masks.append(mask)
        
        # (B, max_F, Q, D) í˜•íƒœë¡œ ë³€í™˜
        video_features = torch.stack(padded_video_features)
        padding_masks = torch.stack(padding_masks)  # (B, max_F)

        B, max_F, Q, D = video_features.shape  # ì´ì œ ëª¨ë“  ë¹„ë””ì˜¤ì˜ Frame ìˆ˜ê°€ ë™ì¼

        # 5ï¸âƒ£ Queryì™€ batch ë‚´ ëª¨ë“  ë¹„ë””ì˜¤ ê°„ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
        video_features = F.normalize(video_features, dim=-1)  # (B, max_F, Q, D)
        scores = torch.einsum('md,bfqd->bfq', text_feat, video_features)  # (B, max_F, Q)

        # 6ï¸âƒ£ Q ì°¨ì›ì—ì„œ ìµœëŒ“ê°’ì„ ì·¨í•´ì„œ frame-level ìœ ì‚¬ë„ ë§Œë“¤ê¸°
        scores, _ = scores.max(dim=-1)  # (B, max_F)

        # 7ï¸âƒ£ íŒ¨ë”©ëœ ë¶€ë¶„ì„ ì œì™¸í•˜ê¸° ìœ„í•´ mask ì ìš©
        scores = scores * padding_masks  # (B, max_F), íŒ¨ë”©ëœ ë¶€ë¶„ì„ 0ìœ¼ë¡œ

        # 8ï¸âƒ£ ë§ˆìŠ¤í¬ ìƒì„± ë° ì •ê·œí™” (ìˆ˜ì •ëœ ë²„ì „)
        masks = scores > (0.2 if hyperparams['is_blip2'] else 0)
        scores *= masks
        
        # ğŸ”Ÿ proposals ìƒì„±
        batch_results = []
        for i in range(B):
            vid, duration, scene_segments = video_meta[i]
            num_frames = frame_counts[i]  # ì›ë˜ ë¹„ë””ì˜¤ì˜ Frame ìˆ˜ ì‚¬ìš©

            cum_scores = torch.cumsum(scores[i][:num_frames], dim=0)

            final_proposals, final_proposals_score, final_proposals_inner_mean = get_proposals_with_scores_batch(
                scene_segments, cum_scores, num_frames, hyperparams['prior']
            )

            final_proposals = torch.tensor(final_proposals)
            final_proposals_score = torch.tensor(final_proposals_score)
            final_proposals_inner_mean = torch.tensor(final_proposals_inner_mean)
            _, score_index = final_proposals_score.sort(descending=True)
            final_proposals = final_proposals[score_index]
            final_proposals_scores = final_proposals_score[score_index]
            final_proposals_inner_mean = final_proposals_inner_mean[score_index]

            # ì •ê·œí™”ëœ proposal ìƒì„±
            if len(final_proposals) == 0:
                static_pred = np.array([[0.0, 0.0], [0.0, 0.0], [0.0, 0.0]])
                proposal_scores  = np.array([1.0, 1.0, 1.0])
            else:
                static_pred = final_proposals / num_frames * duration
                proposal_scores  = final_proposals_scores
                proposal_scores  = proposal_scores  / proposal_scores .max()

            proposals = []
            for s_pred, p_score, inner_mean in zip(static_pred, proposal_scores , final_proposals_inner_mean):
                proposals.append([
                    float(s_pred[0]),
                    float(s_pred[1]),
                    float(p_score),
                    float(inner_mean)
                ])

            proposals = proposals[:hyperparams['cand_num']]

            def calc_iou(candidates, gt):
                start, end = candidates[:, 0], candidates[:, 1]
                s, e = gt[0], gt[1]
                inter = np.minimum(end, e) - np.maximum(start, s)
                union = np.maximum(end, e) - np.minimum(start, s)
                return inter.clip(min=0) / union
            
            def select_proposal(inputs, gamma=0.6):
                weights = inputs[:, 2].clip(min=0)
                proposals = inputs[:, :2]
                scores = np.zeros_like(weights)

                for j in range(scores.shape[0]):
                    iou = calc_iou(proposals, proposals[j])
                    scores[j] += (iou ** gamma * weights).sum()

                idx = np.argsort(-scores)
                return inputs[idx]

            proposals = select_proposal(np.array(proposals))
            batch_results.append({
                'vid': vid,
                'score': proposals[0][-1]
            })

        # 11ï¸âƒ£ í˜„ì¬ batchì˜ ê²°ê³¼ ì €ì¥
        all_results.extend(batch_results)
    
    # 12ï¸âƒ£ ìµœì¢… ì •ë ¬ ë° ë°˜í™˜
    sorted_vids = [item['vid'] for item in sorted(all_results, key=lambda x: x['score'], reverse=True)]
    return sorted_vids


import itertools

def video_retrieval_batch2(data, feature_path, query, stride, hyperparams, batch_size=32):
    all_results = []

    # 1ï¸âƒ£ Queryë¥¼ BLIP-2 ëª¨ë¸ë¡œ ì²˜ë¦¬ (í•œ ë²ˆë§Œ ìˆ˜í–‰)
    with torch.no_grad():
        text = model.tokenizer(query, padding='max_length', truncation=True, max_length=35, return_tensors="pt").to('cuda')
        text_output = model.Qformer.bert(text.input_ids, attention_mask=text.attention_mask, return_dict=True)
        text_feat = model.text_proj(text_output.last_hidden_state[:, 0, :])  # (1, D)
        text_feat = F.normalize(text_feat, dim=-1)  # (1, D)

    # 2ï¸âƒ£ ë°ì´í„°ì…‹ì„ batch_size ë‹¨ìœ„ë¡œ ì²˜ë¦¬
    data_items = list(data.items())
    num_batches = (len(data_items) + batch_size - 1) // batch_size  # ì˜¬ë¦¼ ì²˜ë¦¬

    for batch_idx in range(num_batches):
        batch_data = data_items[batch_idx * batch_size : (batch_idx + 1) * batch_size]
        video_features = []
        video_meta = []  # (vid, duration, segment_boundaries)
        frame_counts = []

        # 3ï¸âƒ£ í˜„ì¬ batchì˜ ë¹„ë””ì˜¤ feature ë¡œë“œ
        for vid, ann in batch_data:
            feature = np.load(os.path.join(feature_path, vid + '.npy'))  # (F, Q, D)
            video_features.append(torch.tensor(feature, dtype=torch.float32, device='cuda'))
            frame_counts.append(feature.shape[0])  # ê° ë¹„ë””ì˜¤ì˜ ì‹¤ì œ Frame ìˆ˜ ì €ì¥
            video_meta.append((vid, ann['duration'], ann['segment_boundaries']))

        # 4ï¸âƒ£ Padding: ê°€ì¥ ê¸´ ë¹„ë””ì˜¤ì˜ Frame ìˆ˜ì— ë§ì¶¤
        max_F = max(frame_counts)  # ë°°ì¹˜ ë‚´ ê°€ì¥ ê¸´ ë¹„ë””ì˜¤ì˜ Frame ìˆ˜
        padded_video_features = []
        padding_masks = []

        for i, feature in enumerate(video_features):
            F_c, Q, D = feature.shape
            pad_size = max_F - F_c

            # ë’¤ìª½ì— íŒ¨ë”© ì¶”ê°€ (zero-padding)
            padded_feature = F.pad(feature, (0, 0, 0, 0, 0, pad_size), mode='constant', value=0)
            padded_video_features.append(padded_feature)

            # ì‹¤ì œ í”„ë ˆì„ ì—¬ë¶€ë¥¼ ë‚˜íƒ€ë‚´ëŠ” mask ìƒì„±
            mask = torch.cat([torch.ones(F_c, device='cuda'), torch.zeros(pad_size, device='cuda')])
            padding_masks.append(mask)

        # (B, max_F, Q, D) í˜•íƒœë¡œ ë³€í™˜
        video_features = torch.stack(padded_video_features)
        padding_masks = torch.stack(padding_masks)  # (B, max_F)

        B, max_F, Q, D = video_features.shape  # ì´ì œ ëª¨ë“  ë¹„ë””ì˜¤ì˜ Frame ìˆ˜ê°€ ë™ì¼

        # 5ï¸âƒ£ Queryì™€ batch ë‚´ ëª¨ë“  ë¹„ë””ì˜¤ ê°„ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
        video_features = F.normalize(video_features, dim=-1)  # (B, max_F, Q, D)
        scores = torch.einsum('md,bfqd->bfq', text_feat, video_features)  # (B, max_F, Q)

        # 6ï¸âƒ£ Q ì°¨ì›ì—ì„œ ìµœëŒ“ê°’ì„ ì·¨í•´ì„œ frame-level ìœ ì‚¬ë„ ë§Œë“¤ê¸°
        scores, _ = scores.max(dim=-1)  # (B, max_F)

        # 7ï¸âƒ£ íŒ¨ë”©ëœ ë¶€ë¶„ì„ ì œì™¸í•˜ê¸° ìœ„í•´ mask ì ìš©
        scores = scores * padding_masks  # (B, max_F), íŒ¨ë”©ëœ ë¶€ë¶„ì„ 0ìœ¼ë¡œ

        # 8ï¸âƒ£ ë§ˆìŠ¤í¬ ìƒì„± ë° ì •ê·œí™”
        masks = scores > (0.2 if hyperparams['is_blip2'] else 0)
        scores *= masks.float()  # (B, max_F)

        # ğŸ”¥ 9ï¸âƒ£ Batch-wise intra score mean ê³„ì‚° (segment_boundaries ê¸°ë°˜)
        # â‘  segment_boundariesì˜ ëª¨ë“  boundaryë¥¼ ë¯¸ë¦¬ ì¶”ì¶œ
        segment_boundaries_list = []
        max_boundaries = 0

        for i in range(B):
            segment_boundaries = video_meta[i][2]  # segment_boundaries ë¦¬ìŠ¤íŠ¸
            segment_boundaries_list.append(segment_boundaries)
            max_boundaries = max(max_boundaries, len(segment_boundaries))

        # â‘¡ ëª¨ë“  ë¹„ë””ì˜¤ì˜ `segment_boundaries`ë¥¼ ë™ì¼í•œ í¬ê¸°ë¡œ íŒ¨ë”© (Batch-wise ì—°ì‚° ìœ„í•´)
        segment_boundaries_tensor = torch.zeros((B, max_boundaries), dtype=torch.long, device='cuda')
        for i in range(B):
            segment_boundaries_tensor[i, :len(segment_boundaries_list[i])] = torch.tensor(segment_boundaries_list[i], dtype=torch.long, device='cuda')

        # â‘¢ ê°€ëŠ¥í•œ ëª¨ë“  (start, end) ì¡°í•©ì„ ë¯¸ë¦¬ ìƒì„±
        combs = list(itertools.combinations(range(max_boundaries), 2))
        start_idx_tensor = segment_boundaries_tensor[:, [c[0] for c in combs]]
        end_idx_tensor = segment_boundaries_tensor[:, [c[1] for c in combs]]  # (B, num_combinations)

        # â‘£ Batch-wise intra mean score ê³„ì‚° (GPU ì—°ì‚°)
        intra_scores = torch.zeros((B, len(combs)), dtype=torch.float32, device='cuda')

        for j in range(len(combs)):  # ëª¨ë“  ì¡°í•©ì— ëŒ€í•´ í•œ ë²ˆì”©ë§Œ ì—°ì‚°
            start = start_idx_tensor[:, j]
            end = end_idx_tensor[:, j]
            valid_mask = start < end  # ìœ íš¨í•œ start-end ì¡°í•©ë§Œ ì—°ì‚°
            intra_scores[:, j] = torch.where(valid_mask, torch.stack([scores[i, start[i]:end[i]].mean() for i in range(B)]), torch.tensor(0.0, device='cuda'))

        # ğŸ”¥ 10ï¸âƒ£ ê° ë¹„ë””ì˜¤ì—ì„œ ê°€ì¥ ë†’ì€ intra score meanë§Œ ì €ì¥
        max_intra_scores, max_indices = intra_scores.max(dim=1)  # (B,)
        best_start_indices = start_idx_tensor[torch.arange(B, device='cuda'), max_indices]
        best_end_indices = end_idx_tensor[torch.arange(B, device='cuda'), max_indices]
        
        # ğŸš€ ìµœì¢… ê²°ê³¼ ì €ì¥
        for i in range(B):
            all_results.append({
                'vid': video_meta[i][0],  # ë¹„ë””ì˜¤ ID
                'best_segment': (best_start_indices[i].item(), best_end_indices[i].item()),  # ìµœê³  intra score meanì„ ê°€ì§„ segment
                'best_intra_score_mean': max_intra_scores[i].item()  # ìµœê³  intra score mean ê°’
            })

    sorted_vids = [item['vid'] for item in sorted(all_results, key=lambda x: x['best_intra_score_mean'], reverse=True)]
    return sorted_vids